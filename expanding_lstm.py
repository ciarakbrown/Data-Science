# -*- coding: utf-8 -*-
"""expanding_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1llQy31urYFTaVOI1T0dQL3mDYFyElseU
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import os
import glob
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import cycle
import tempfile
from google.colab import drive

np.random.seed(42)
tf.random.set_seed(42)

def load_data(path):
    """
    Loads patient data from PSV files and returns the full dataset with unique patient IDs.
    This function ensures that if multiple files share the same filename (basename),
    only the first encountered file is loaded.
    """
    # Get all PSV file paths
    files = glob.glob(os.path.join(path, '*.psv'))

    # Create a dictionary to hold the first occurrence for each basename.
    unique_files = {}
    for file in files:
        base_name = os.path.basename(file)
        if base_name not in unique_files:
            unique_files[base_name] = file

    # Optionally, print out the number of unique files detected
    print("Unique patient files found:", len(unique_files))

    # Now load only the unique files.
    data_frames = []
    for base_name, file in unique_files.items():
        df = pd.read_csv(file, sep='|')
        df['patient_id'] = base_name  # Use the filename as patient_id
        data_frames.append(df)

    full_dataset = pd.concat(data_frames, ignore_index=True)
    full_dataset = full_dataset.drop(columns=['HospAdmTime'], errors='ignore')
    return full_dataset

# load dataset and print first few lines to check

path = "/content/drive/MyDrive/cleaned_dataset"
drive.mount('/content/drive')
df = load_data(path)
df = df.drop(columns=['Age_18-44', 'Age_45-59', 'Age_60-64', 'Age_65-74', 'Age_75-79', 'Age_80-89'], errors='ignore')
print(df.head())

# ----------------------------------------------------------------------------
# Plot feature distributions for continuous features only
# ----------------------------------------------------------------------------
df2 = df.drop(columns=['SepsisLabel', 'Gender', 'Age_18-44', 'Age_45-59', 'Age_60-64', 'Age_65-74', 'Age_75-79', 'Age_80-89'], errors='ignore')
df_melt = df2.melt(id_vars=['patient_id'], var_name='feature', value_name='value')
# Create the violin plot
plt.figure(figsize=(12, 6))
sns.violinplot(x='feature', y='value', data=df_melt, inner='box')
plt.title("Violin Plot of Patient Features (All Rows)")
plt.xlabel("Feature")
plt.ylabel("Value")
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

# ----------------------------------------------------------------------------
# Normalise continuous features, train-test split and plot normalised features
# ----------------------------------------------------------------------------
df_continuous = df.drop(
    columns=[
        'patient_id', 'SepsisLabel', 'Gender',
        'Age_18-44', 'Age_45-59', 'Age_60-64', 'Age_65-74', 'Age_75-79', 'Age_80-89'
    ],
    errors='ignore'
)

# select only contiuous features
continuous_cols = df_continuous.columns
unique_ids = df['patient_id'].unique()
np.random.shuffle(unique_ids)

train_size = int(0.8 * len(unique_ids))
test_size   = int(0.2 * len(unique_ids))

train_ids = unique_ids[:train_size]
test_ids   = unique_ids[train_size:train_size + test_size]

# patient - level train test split
train_df = df[df['patient_id'].isin(train_ids)].copy()
test_df   = df[df['patient_id'].isin(test_ids)].copy()

# calculate stats
train_mean = train_df[continuous_cols].mean()
train_std  = train_df[continuous_cols].std()

# apply z score normalisation
train_df[continuous_cols] = (train_df[continuous_cols] - train_mean) / train_std
test_df[continuous_cols]  = (test_df[continuous_cols]  - train_mean) / train_std

df_continuous_std = (df[continuous_cols] - train_mean) / train_std
df_continuous_std = df_continuous_std.melt(var_name='Column', value_name='Normalized')

plt.figure(figsize=(12, 6))
ax = sns.violinplot(x='Column', y='Normalized', data=df_continuous_std)
_ = ax.set_xticklabels(df_continuous_std['Column'].unique(), rotation=90)
plt.show()

# ----------------------------------------------------------------------------
# EARLY LABEL SHIFTING
# Moves first 1 sepsis label to N hours before to give early warning
# ----------------------------------------------------------------------------

def apply_label_shift(df, shift_hours=4):
    df = df.copy()
    new_labels = []

    for pid in df['patient_id'].unique():
        patient_df = df[df['patient_id'] == pid]
        sepsis_indices = patient_df.index[patient_df['SepsisLabel'] == 1].tolist()

        if sepsis_indices:
            first_sepsis = sepsis_indices[0]
            shifted_index = max(patient_df.index.min(), first_sepsis - shift_hours)
            df.loc[shifted_index:first_sepsis - 1, 'SepsisLabel'] = 1  # shift earlier

    return df

# ----------------------------------------------------------------------------
# CREATE SHIFTED TRAINING DATA
# ----------------------------------------------------------------------------

train_df_shifted = apply_label_shift(train_df, shift_hours=4)
print(train_df_shifted['SepsisLabel'].value_counts())

# ----------------------------------------------------------------------------
# CREATE DATASET WITHE EXPANDING WINDOW AND PADDING
# ----------------------------------------------------------------------------

from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

def create_expanding_dataset(
    df,
    feature_cols,
    min_window=6,
    max_window=None,
    padding='post'
):
    """
    Generates expanding window sequences for LSTM training/testing.

    Parameters:
    - df: DataFrame with 'patient_id', 'SepsisLabel', and feature columns
    - feature_cols: List of column names to use as input features
    - min_window: Minimum number of time steps to start expanding from
    - max_window: Optional max number of time steps (defaults to full length)
    - padding: 'pre' or 'post' — how to pad sequences

    Returns:
    - X_padded: padded 3D array (samples, timesteps, features)
    - y_array: binary labels, one per window
    """
    X, y = [], []

    for pid in df['patient_id'].unique():
        patient_df = df[df['patient_id'] == pid]
        features = patient_df[feature_cols].values
        labels   = patient_df['SepsisLabel'].values

        max_len = len(features) if max_window is None else min(len(features), max_window)

        for t in range(min_window, max_len):
            X.append(features[:t])       # expanding input window
            y.append(labels[t])          # label at current time

    if not X or not y:
        return np.array([]), np.array([])

    X_padded = pad_sequences(X, padding=padding, dtype='float32')
    y_array  = np.array(y)

    return X_padded, y_array


# ----------------------------------------------------------------------------
# FUNCTION TO CONVERT TO DATAFRAMES
# ----------------------------------------------------------------------------

def create_datasets(X, y, batch_size, shuffle=True):
    """
    Converts NumPy arrays into a TensorFlow dataset with batching and optional shuffling.
    """
    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.float32)

    ds = tf.data.Dataset.from_tensor_slices((X, y))

    if shuffle:
        ds = ds.shuffle(buffer_size=len(X))

    ds = ds.batch(batch_size)
    return ds

# ----------------------------------------------------------------------------
# LSTM MODEL ARCHITECTURE
# ----------------------------------------------------------------------------

def make_lstm_model_single_output(window_size, num_features, dropout_rate, l2_reg, learning_rate):
    model = tf.keras.Sequential([
        tf.keras.layers.Masking(mask_value=0., input_shape=(window_size, num_features)),
        tf.keras.layers.LSTM(64, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),
        tf.keras.layers.Dropout(dropout_rate),
        tf.keras.layers.LSTM(32, return_sequences=False, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),
        tf.keras.layers.Dropout(dropout_rate),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.AUC(name='prc', curve='PR')
        ]
    )
    return model

# ----------------------------------------------------------------------------
# BAYESIAN HYPERPARAMETER OPTIMISATION
# ----------------------------------------------------------------------------
# !pip install scikit-optimize
from skopt import gp_minimize
from skopt.space import Integer
from skopt.utils import use_named_args
from skopt.space import Integer, Real
import numpy as np
import tensorflow as tf
from sklearn.metrics import roc_auc_score

def bayesian_optimise_expanding_lstm(
    train_df,
    test_df,
    feature_cols,
    min_window_bounds=(6, 15),
    batch_bounds=(64, 512),
    n_calls=15,
    random_state=42,
    verbose=1,
):
    dimensions = [
        Integer(*min_window_bounds, name="min_window"),
        Integer(*batch_bounds, name="batch_size"),
        Real(0.2, 0.5, name="dropout"),               # Dropout between 20% and 50%
        Real(1e-5, 1e-2, name="learning_rate", prior="log-uniform"),
        Real(1e-5, 1e-2, name="l2_reg", prior="log-uniform")
    ]


    best_score, best_model, best_params = -np.inf, None, None

    @use_named_args(dimensions)
    def objective(min_window, batch_size, dropout, learning_rate, l2_reg):
        nonlocal best_score, best_model, best_params

        if verbose:
            print(f"\n🔍 Trying min_window={min_window}, batch_size={batch_size}, dropout={dropout:.2f}, lr={learning_rate:.5f}, l2={l2_reg:.5f}")

        X_train, y_train = create_expanding_dataset(train_df, feature_cols, min_window=min_window)
        X_test, y_test = create_expanding_dataset(test_df, feature_cols, min_window=min_window)

        if len(X_train) < 100 or np.sum(y_train) < 10:
            print("⚠️ Skipping due to insufficient data.")
            return 1.0

        train_ds = create_datasets(X_train, y_train, batch_size=batch_size, shuffle=True)
        test_ds = create_datasets(X_test, y_test, batch_size=batch_size, shuffle=False)

        timesteps = X_train.shape[1]
        num_features = X_train.shape[2]

        model = tf.keras.Sequential([
            tf.keras.layers.Masking(mask_value=0., input_shape=(timesteps, num_features)),
            tf.keras.layers.LSTM(64, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),
            tf.keras.layers.Dropout(dropout),
            tf.keras.layers.LSTM(32, return_sequences=False, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),
            tf.keras.layers.Dropout(dropout),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss='binary_crossentropy',
            metrics=[
                'accuracy',
                tf.keras.metrics.AUC(name='auc'),
                tf.keras.metrics.Recall(name='recall'),
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.AUC(name='prc', curve='PR')
            ]
        )

        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_auc', patience=5, mode='max', restore_best_weights=True
        )

        model.fit(train_ds, validation_data=test_ds, epochs=20, callbacks=[early_stopping], verbose=0)

        y_pred = model.predict(X_test).ravel()
        auc = roc_auc_score(y_test, y_pred)

        if auc > best_score:
            best_score = auc
            best_model = model
            best_params = {
                "min_window": min_window,
                "batch_size": batch_size,
                "dropout": dropout,
                "learning_rate": learning_rate,
                "l2_reg": l2_reg
            }

        print(f"✅ AUC: {auc:.4f}")
        return -auc

    # Start optimisation
    print("\n🎯 Starting Bayesian Optimisation...")
    gp_minimize(
        objective,
        dimensions,
        n_calls=n_calls,
        n_random_starts=5,
        random_state=random_state,
        verbose=verbose
    )

    print(f"\n🏆 Best config: {best_params}, AUC: {best_score:.4f}")
    return best_params, best_model

# run the optimiser

best_params, best_model = bayesian_optimise_expanding_lstm(
    train_df=train_df_shifted,
    test_df=test_df,
    feature_cols=continuous_cols,
    min_window_bounds=(6, 15),
    batch_bounds=(64, 256),
    n_calls=15,
    verbose=1
)

# ----------------------------------------------------------------------------
# RUN FINAL LSTM MODEL AND PRINT RESULTS
# ----------------------------------------------------------------------------
X_train_final, y_train_final = create_expanding_dataset(
            df=train_df_shifted,
            feature_cols=continuous_cols,
            min_window=best_params["min_window"],
        )

X_test_final, y_test_final = create_expanding_dataset(
            df=test_df,
            feature_cols=continuous_cols,
            min_window=best_params["min_window"],
        )

# Create TensorFlow datasets
BATCH_SIZE = best_params["batch_size"]
dropout_rate = best_params["dropout_rate"]
l2_reg = best_params["l2_reg"]
learning_rate = best_params["learning_rate"]

train_ds = create_datasets(X_train_final, y_train_final, batch_size=BATCH_SIZE, shuffle=True)
test_ds  = create_datasets(X_test_final, y_test_final, batch_size=BATCH_SIZE, shuffle=False)

# Compute class weights for imbalance
pos_count = np.sum(y_train_final == 1)
neg_count = np.sum(y_train_final == 0)
total_windows = len(y_train_final)

weight_for_0 = (1 / neg_count) * (total_windows / 2.0)
weight_for_1 = (1 / pos_count) * (total_windows / 2.0)
class_weight = {0: weight_for_0, 1: weight_for_1}

# Early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_recall',
    patience=10,
    mode='max',
    restore_best_weights=True
)

# Build model
timesteps = X_train_final.shape[1]
num_features = X_train_final.shape[2]
model = make_lstm_model_single_output(timesteps, num_features, dropout_rate, l2_reg, learning_rate)

# Train model
history = model.fit(
    train_ds,
    validation_data=test_ds,
    epochs=50,
    callbacks=[early_stopping],
    class_weight=class_weight
)


# ----------------------------------------------------------------------------
# SAVE LABEL + PREDICTION FILES FOR EVALUATION
# ----------------------------------------------------------------------------
import os
import pandas as pd
from tensorflow.keras.preprocessing.sequence import pad_sequences

def save_predictions_per_patient(
    model,
    test_df,
    feature_cols,
    min_window,
    label_dir="labels",
    pred_dir="predictions"
):
    os.makedirs(label_dir, exist_ok=True)
    os.makedirs(pred_dir, exist_ok=True)

    for pid, df_p in test_df.groupby("patient_id"):
        df_p = df_p.copy()
        features = df_p[feature_cols].values
        labels = df_p["SepsisLabel"].values

        preds = []
        probs = []

        for t in range(min_window, len(df_p)):
            window = features[:t]
            padded = pad_sequences([window], maxlen=len(df_p), dtype='float32', padding='post')
            prob = model.predict(padded, verbose=0)[0][0]
            pred = int(prob >= 0.5)
            probs.append(prob)
            preds.append(pred)

        # Pad beginning with zeros to align to full patient length
        pad_len = min_window
        probs = [0.0]*pad_len + probs
        preds = [0]*pad_len + preds

        # Save label
        df_label = pd.DataFrame({"SepsisLabel": labels})
        df_label.to_csv(f"{label_dir}/{pid}.psv", sep='|', index=False)

        # Save prediction
        df_pred = pd.DataFrame({
            "PredictedProbability": probs,
            "PredictedLabel": preds
        })
        df_pred.to_csv(f"{pred_dir}/{pid}.psv", sep='|', index=False)

# Run the function to create evaluation files
save_predictions_per_patient(
    model=model,
    test_df=test_df,
    feature_cols=continuous_cols,
    min_window=best_params["min_window"]
)


# ----------------------------------------------------------------------------
# 📉 Plot Loss and Accuracy
# ----------------------------------------------------------------------------
def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    # Loss
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss', color='blue')
    plt.plot(history.history['val_loss'], label='Val Loss', color='red')
    plt.title('Loss over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.legend()

    # Accuracy
    if 'accuracy' in history.history:
        plt.subplot(1, 2, 2)
        plt.plot(history.history['accuracy'], label='Train Acc', color='blue')
        plt.plot(history.history['val_accuracy'], label='Val Acc', color='red')
        plt.title('Accuracy over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.grid(True)
        plt.legend()

    plt.tight_layout()
    plt.show()

# ----------------------------------------------------------------------------
# 📈 Plot Recall over Epochs
# ----------------------------------------------------------------------------
def plot_recall_history(history):
    plt.figure(figsize=(6, 5))
    plt.plot(history.history['recall'], label='Train Recall', color='blue')
    plt.plot(history.history['val_recall'], label='Val Recall', color='red')
    plt.title('Recall Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Recall')
    plt.legend()
    plt.grid(True)
    plt.show()

# ----------------------------------------------------------------------------
# 📉 Plot ROC Curve
# ----------------------------------------------------------------------------
def plot_roc_curve(model, X_test, y_test):
    y_pred_proba = model.predict(X_test).ravel()
    fpr, tpr, _ = roc_curve(y_test.ravel(), y_pred_proba)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f"ROC curve (AUC = {roc_auc:.4f})")
    plt.plot([0, 1], [0, 1], color='red', lw=1, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.show()

# ----------------------------------------------------------------------------
# 🔲 Confusion Matrix
# ----------------------------------------------------------------------------
def plot_confusion_matrix(model, X_test, y_test, threshold=0.5):
    y_pred_proba = model.predict(X_test).ravel()
    y_pred = (y_pred_proba >= threshold).astype(int)

    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)
    plt.title(f'Confusion Matrix (Threshold = {threshold})')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))

# ----------------------------------------------------------------------------
# 💡 Run All Plots
# ----------------------------------------------------------------------------
plot_training_history(history)
plot_recall_history(history)
plot_roc_curve(model, X_test_final, y_test_final)
plot_confusion_matrix(model, X_test_final, y_test_final, threshold=0.5)

# ----------------------------------------------------------------------------
#EVALUATION
# ----------------------------------------------------------------------------


#!/usr/bin/env python

# This file contains functions for evaluating algorithms for the 2019 PhysioNet/
# CinC Challenge. You can run it as follows:
#
#   python evaluate_sepsis_score.py labels predictions scores.psv
#
# where 'labels' is a directory containing files with labels, 'predictions' is a
# directory containing files with predictions, and 'scores.psv' (optional) is a
# collection of scores for the predictions.

################################################################################

# The evaluate_scores function computes a normalized utility score for a cohort
# of patients along with several traditional scoring metrics.
#
# Inputs:
#   'label_directory' is a directory of pipe-delimited text files containing a
#   binary vector of labels for whether a patient is not septic (0) or septic
#   (1) for each time interval.
#
#   'prediction_directory' is a directory of pipe-delimited text files, where
#   the first column of the file gives the predicted probability that the
#   patient is septic at each time, and the second column of the file is a
#   binarized version of this vector. Note that there must be a prediction for
#   every label.
#
# Outputs:
#   'auroc' is the area under the receiver operating characteristic curve
#   (AUROC).
#
#   'auprc' is the area under the precision recall curve (AUPRC).
#
#   'accuracy' is accuracy.
#
#   'f_measure' is F-measure.
#
#   'normalized_observed_utility' is a normalized utility-based measure that we
#   created for the Challenge. This score is normalized so that a perfect score
#   is 1 and no positive predictions is 0.
#
# Example:
#   Omitted due to length. See the below examples.

import numpy as np, os, os.path, sys, warnings

def evaluate_sepsis_score(label_directory, prediction_directory):
    # Set parameters.
    label_header       = 'SepsisLabel'
    prediction_header  = 'PredictedLabel'
    probability_header = 'PredictedProbability'

    dt_early   = -12
    dt_optimal = -6
    dt_late    = 3

    max_u_tp = 1
    min_u_fn = -2
    u_fp     = -0.05
    u_tn     = 0

    # Find label and prediction files.
    label_files = []
    for f in os.listdir(label_directory):
        g = os.path.join(label_directory, f)
        if os.path.isfile(g) and not f.lower().startswith('.') and f.lower().endswith('psv'):
            label_files.append(g)
    label_files = sorted(label_files)

    prediction_files = []
    for f in os.listdir(prediction_directory):
        g = os.path.join(prediction_directory, f)
        if os.path.isfile(g) and not f.lower().startswith('.') and f.lower().endswith('psv'):
            prediction_files.append(g)
    prediction_files = sorted(prediction_files)

    if len(label_files) != len(prediction_files):
        raise Exception('Numbers of label and prediction files must be the same.')

    # Load labels and predictions.
    num_files            = len(label_files)
    cohort_labels        = []
    cohort_predictions   = []
    cohort_probabilities = []

    for k in range(num_files):
        labels        = load_column(label_files[k], label_header, '|')
        predictions   = load_column(prediction_files[k], prediction_header, '|')
        probabilities = load_column(prediction_files[k], probability_header, '|')

        # Check labels and predictions for errors.
        if not (len(labels) == len(predictions) and len(predictions) == len(probabilities)):
            raise Exception('Numbers of labels and predictions for a file must be the same.')

        num_rows = len(labels)

        for i in range(num_rows):
            if labels[i] not in (0, 1):
                raise Exception('Labels must satisfy label == 0 or label == 1.')

            if predictions[i] not in (0, 1):
                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')

            if not 0 <= probabilities[i] <= 1:
                warnings.warn('Probabilities do not satisfy 0 <= probability <= 1.')

        if 0 < np.sum(predictions) < num_rows:
            min_probability_positive = np.min(probabilities[predictions == 1])
            max_probability_negative = np.max(probabilities[predictions == 0])

            if min_probability_positive <= max_probability_negative:
                warnings.warn('Predictions are inconsistent with probabilities, i.e., a positive prediction has a lower (or equal) probability than a negative prediction.')

        # Record labels and predictions.
        cohort_labels.append(labels)
        cohort_predictions.append(predictions)
        cohort_probabilities.append(probabilities)

    # Compute AUC, accuracy, and F-measure.
    labels        = np.concatenate(cohort_labels)
    predictions   = np.concatenate(cohort_predictions)
    probabilities = np.concatenate(cohort_probabilities)

    auroc, auprc        = compute_auc(labels, probabilities)
    accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)

    # Compute utility.
    observed_utilities = np.zeros(num_files)
    best_utilities     = np.zeros(num_files)
    worst_utilities    = np.zeros(num_files)
    inaction_utilities = np.zeros(num_files)

    for k in range(num_files):
        labels = cohort_labels[k]
        num_rows          = len(labels)
        observed_predictions = cohort_predictions[k]
        best_predictions     = np.zeros(num_rows)
        worst_predictions    = np.zeros(num_rows)
        inaction_predictions = np.zeros(num_rows)

        if np.any(labels):
            t_sepsis = np.argmax(labels) - dt_optimal
            best_predictions[max(0, t_sepsis + dt_early) : min(t_sepsis + dt_late + 1, num_rows)] = 1
        worst_predictions = 1 - best_predictions

        observed_utilities[k] = compute_prediction_utility(labels, observed_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)
        best_utilities[k]     = compute_prediction_utility(labels, best_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)
        worst_utilities[k]    = compute_prediction_utility(labels, worst_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)
        inaction_utilities[k] = compute_prediction_utility(labels, inaction_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)

    unnormalized_observed_utility = np.sum(observed_utilities)
    unnormalized_best_utility     = np.sum(best_utilities)
    unnormalized_worst_utility    = np.sum(worst_utilities)
    unnormalized_inaction_utility = np.sum(inaction_utilities)

    normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (unnormalized_best_utility - unnormalized_inaction_utility)

    return auroc, auprc, accuracy, f_measure, normalized_observed_utility

# The load_column function loads a column from a table.
#
# Inputs:
#   'filename' is a string containing a filename.
#
#   'header' is a string containing a header.
#
# Outputs:
#   'column' is a vector containing a column from the file with the given
#   header.
#
# Example:
#   Omitted.

def load_column(filename, header, delimiter):
    column = []
    with open(filename, 'r') as f:
        for i, l in enumerate(f):
            arrs = l.strip().split(delimiter)
            if i == 0:
                try:
                    j = arrs.index(header)
                except:
                    raise Exception('{} must contain column with header {} containing numerical entries.'.format(filename, header))
            else:
                if len(arrs[j]):
                    column.append(float(arrs[j]))
    return np.array(column)

# The compute_auc function computes AUROC and AUPRC as well as other summary
# statistics (TP, FP, FN, TN, TPR, TNR, PPV, NPV, etc.) that can be exposed
# from this function.
#
# Inputs:
#   'labels' is a binary vector, where labels[i] == 0 if the patient is not
#   labeled as septic at time i and labels[i] == 1 if the patient is labeled as
#   septic at time i.
#
#   'predictions' is a probability vector, where predictions[i] gives the
#   predicted probability that the patient is septic at time i.  Note that there
#   must be a prediction for every label, i.e, len(labels) ==
#   len(predictions).
#
# Outputs:
#   'auroc' is a scalar that gives the AUROC of the algorithm using its
#   predicted probabilities, where specificity is interpolated for intermediate
#   sensitivity values.
#
#   'auprc' is a scalar that gives the AUPRC of the algorithm using its
#   predicted probabilities, where precision is a piecewise constant function of
#   recall.
#
# Example:
#   In [1]: labels = [0, 0, 0, 0, 1, 1]
#   In [2]: predictions = [0.3, 0.4, 0.6, 0.7, 0.8, 0.8]
#   In [3]: auroc, auprc = compute_auc(labels, predictions)
#   In [4]: auroc
#   Out[4]: 1.0
#   In [5]: auprc
#   Out[5]: 1.0

def compute_auc(labels, predictions, check_errors=True):
    # Check inputs for errors.
    if check_errors:
        if len(predictions) != len(labels):
            raise Exception('Numbers of predictions and labels must be the same.')

        for label in labels:
            if not label in (0, 1):
                raise Exception('Labels must satisfy label == 0 or label == 1.')

        for prediction in predictions:
            if not 0 <= prediction <= 1:
                warnings.warn('Predictions do not satisfy 0 <= prediction <= 1.')

    # Find prediction thresholds.
    thresholds = np.unique(predictions)[::-1]
    if thresholds[0] != 1:
        thresholds = np.insert(thresholds, 0, 1)
    if thresholds[-1] == 0:
        thresholds = thresholds[:-1]

    n = len(labels)
    m = len(thresholds)

    # Populate contingency table across prediction thresholds.
    tp = np.zeros(m)
    fp = np.zeros(m)
    fn = np.zeros(m)
    tn = np.zeros(m)

    # Find indices that sort the predicted probabilities from largest to
    # smallest.
    idx = np.argsort(predictions)[::-1]

    i = 0
    for j in range(m):
        # Initialize contingency table for j-th prediction threshold.
        if j == 0:
            tp[j] = 0
            fp[j] = 0
            fn[j] = np.sum(labels)
            tn[j] = n - fn[j]
        else:
            tp[j] = tp[j - 1]
            fp[j] = fp[j - 1]
            fn[j] = fn[j - 1]
            tn[j] = tn[j - 1]

        # Update contingency table for i-th largest predicted probability.
        while i < n and predictions[idx[i]] >= thresholds[j]:
            if labels[idx[i]]:
                tp[j] += 1
                fn[j] -= 1
            else:
                fp[j] += 1
                tn[j] -= 1
            i += 1

    # Summarize contingency table.
    tpr = np.zeros(m)
    tnr = np.zeros(m)
    ppv = np.zeros(m)
    npv = np.zeros(m)

    for j in range(m):
        if tp[j] + fn[j]:
            tpr[j] = tp[j] / (tp[j] + fn[j])
        else:
            tpr[j] = 1
        if fp[j] + tn[j]:
            tnr[j] = tn[j] / (fp[j] + tn[j])
        else:
            tnr[j] = 1
        if tp[j] + fp[j]:
            ppv[j] = tp[j] / (tp[j] + fp[j])
        else:
            ppv[j] = 1
        if fn[j] + tn[j]:
            npv[j] = tn[j] / (fn[j] + tn[j])
        else:
            npv[j] = 1

    # Compute AUROC as the area under a piecewise linear function with TPR /
    # sensitivity (x-axis) and TNR / specificity (y-axis) and AUPRC as the area
    # under a piecewise constant with TPR / recall (x-axis) and PPV / precision
    # (y-axis).
    auroc = 0
    auprc = 0
    for j in range(m-1):
        auroc += 0.5 * (tpr[j + 1] - tpr[j]) * (tnr[j + 1] + tnr[j])
        auprc += (tpr[j + 1] - tpr[j]) * ppv[j + 1]

    return auroc, auprc

# The compute_accuracy_f_measure function computes the accuracy and F-measure
# for a patient.
#
# Inputs:
#   'labels' is a binary vector, where labels[i] == 0 if the patient is not
#   labeled as septic at time i and labels[i] == 1 if the patient is labeled as
#   septic at time i.
#
#   'predictions' is a binary vector, where predictions[i] == 0 if the patient
#   is not predicted to be septic at time i and predictions[i] == 1 if the
#   patient is predicted to be septic at time i.  Note that there must be a
#   prediction for every label, i.e, len(labels) == len(predictions).
#
# Output:
#   'accuracy' is a scalar that gives the accuracy of the predictions using its
#   binarized predictions.
#
#   'f_measure' is a scalar that gives the F-measure of the predictions using its
#   binarized predictions.
#
# Example:
#   In [1]: labels = [0, 0, 0, 0, 1, 1]
#   In [2]: predictions = [0, 0, 1, 1, 1, 1]
#   In [3]: accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)
#   In [4]: accuracy
#   Out[4]: 0.666666666667
#   In [5]: f_measure
#   Out[5]: 0.666666666667

def compute_accuracy_f_measure(labels, predictions, check_errors=True):
    # Check inputs for errors.
    if check_errors:
        if len(predictions) != len(labels):
            raise Exception('Numbers of predictions and labels must be the same.')

        for label in labels:
            if not label in (0, 1):
                raise Exception('Labels must satisfy label == 0 or label == 1.')

        for prediction in predictions:
            if not prediction in (0, 1):
                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')

    # Populate contingency table.
    n = len(labels)
    tp = 0
    fp = 0
    fn = 0
    tn = 0

    for i in range(n):
        if labels[i] and predictions[i]:
            tp += 1
        elif not labels[i] and predictions[i]:
            fp += 1
        elif labels[i] and not predictions[i]:
            fn += 1
        elif not labels[i] and not predictions[i]:
            tn += 1

    # Summarize contingency table.
    if tp + fp + fn + tn:
        accuracy = float(tp + tn) / float(tp + fp + fn + tn)
    else:
        accuracy = 1.0

    if 2 * tp + fp + fn:
        f_measure = float(2 * tp) / float(2 * tp + fp + fn)
    else:
        f_measure = 1.0

    return accuracy, f_measure

# The compute_prediction_utility function computes the total time-dependent
# utility for a patient.
#
# Inputs:
#   'labels' is a binary vector, where labels[i] == 0 if the patient is not
#   labeled as septic at time i and labels[i] == 1 if the patient is labeled as
#   septic at time i.
#
#   'predictions' is a binary vector, where predictions[i] == 0 if the patient
#   is not predicted to be septic at time i and predictions[i] == 1 if the
#   patient is predicted to be septic at time i.  Note that there must be a
#   prediction for every label, i.e, len(labels) == len(predictions).
#
# Output:
#   'utility' is a scalar that gives the total time-dependent utility of the
#   algorithm using its binarized predictions.
#
# Example:
#   In [1]: labels = [0, 0, 0, 0, 1, 1]
#   In [2]: predictions = [0, 0, 1, 1, 1, 1]
#   In [3]: utility = compute_prediction_utility(labels, predictions)
#   In [4]: utility
#   Out[4]: 3.388888888888889

def compute_prediction_utility(labels, predictions, dt_early=-12, dt_optimal=-6, dt_late=3.0, max_u_tp=1, min_u_fn=-2, u_fp=-0.05, u_tn=0, check_errors=True):
    # Check inputs for errors.
    if check_errors:
        if len(predictions) != len(labels):
            raise Exception('Numbers of predictions and labels must be the same.')

        for label in labels:
            if not label in (0, 1):
                raise Exception('Labels must satisfy label == 0 or label == 1.')

        for prediction in predictions:
            if not prediction in (0, 1):
                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')

        if dt_early >= dt_optimal:
            raise Exception('The earliest beneficial time for predictions must be before the optimal time.')

        if dt_optimal >= dt_late:
            raise Exception('The optimal time for predictions must be before the latest beneficial time.')

    # Does the patient eventually have sepsis?
    if np.any(labels):
        is_septic = True
        t_sepsis = np.argmax(labels) - dt_optimal
    else:
        is_septic = False
        t_sepsis = float('inf')

    n = len(labels)

    # Define slopes and intercept points for utility functions of the form
    # u = m * t + b.
    m_1 = float(max_u_tp) / float(dt_optimal - dt_early)
    b_1 = -m_1 * dt_early
    m_2 = float(-max_u_tp) / float(dt_late - dt_optimal)
    b_2 = -m_2 * dt_late
    m_3 = float(min_u_fn) / float(dt_late - dt_optimal)
    b_3 = -m_3 * dt_optimal

    # Compare predicted and true conditions.
    u = np.zeros(n)
    for t in range(n):
        if t <= t_sepsis + dt_late:
            # TP
            if is_septic and predictions[t]:
                if t <= t_sepsis + dt_optimal:
                    u[t] = max(m_1 * (t - t_sepsis) + b_1, u_fp)
                elif t <= t_sepsis + dt_late:
                    u[t] = m_2 * (t - t_sepsis) + b_2
            # FP
            elif not is_septic and predictions[t]:
                u[t] = u_fp
            # FN
            elif is_septic and not predictions[t]:
                if t <= t_sepsis + dt_optimal:
                    u[t] = 0
                elif t <= t_sepsis + dt_late:
                    u[t] = m_3 * (t - t_sepsis) + b_3
            # TN
            elif not is_septic and not predictions[t]:
                u[t] = u_tn

    # Find total utility for patient.
    return np.sum(u)

if __name__ == '__main__':

    LABEL_DIR = '/content/labels'        # folder that holds SepsisLabel .psv
    PRED_DIR  = '/content/predictions'   # folder that holds prediction .psv

    auroc, auprc, accuracy, f_measure, utility = evaluate_sepsis_score(
        LABEL_DIR,
        PRED_DIR
    )

    output_string = (
        'AUROC|AUPRC|Accuracy|F-measure|Utility\n'
        f'{auroc}|{auprc}|{accuracy}|{f_measure}|{utility}'
    )

    OUT_FILE = None

    if OUT_FILE:
        with open(OUT_FILE, 'w') as f:
            f.write(output_string)
        print(f'Results written to {OUT_FILE}')
    else:
        print(output_string)